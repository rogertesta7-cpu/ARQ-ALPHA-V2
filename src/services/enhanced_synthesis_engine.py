#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v4.0 - Enhanced Synthesis Engine
Motor de s√≠ntese aprimorado com busca ativa e an√°lise profunda
"""

import os
import logging
import json
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum

logger = logging.getLogger(__name__)


class SynthesisType(Enum):
    """Tipos de s√≠ntese dispon√≠veis"""
    MASTER = "master_synthesis"
    MARKET = "deep_market_analysis"
    BEHAVIORAL = "behavioral_analysis"
    COMPETITIVE = "competitive_analysis"


@dataclass
class SynthesisMetrics:
    """M√©tricas da s√≠ntese executada"""
    context_size: int
    processing_time: float
    ai_searches: int
    data_sources: int
    confidence_level: float
    timestamp: str


class DataLoadError(Exception):
    """Erro ao carregar dados"""
    pass


class SynthesisExecutionError(Exception):
    """Erro durante execu√ß√£o da s√≠ntese"""
    pass


class EnhancedSynthesisEngine:
    """Motor de s√≠ntese aprimorado com IA e busca ativa"""

    def __init__(self):
        """Inicializa o motor de s√≠ntese"""
        self.synthesis_prompts = self._load_enhanced_prompts()
        self.ai_manager = None
        self._initialize_ai_manager()
        self.metrics_cache = {}
        
        logger.info("üß† Enhanced Synthesis Engine v4.0 inicializado")

    def _initialize_ai_manager(self) -> None:
        """Inicializa o gerenciador de IA com hierarquia OpenRouter"""
        try:
            from services.enhanced_ai_manager import enhanced_ai_manager
            self.ai_manager = enhanced_ai_manager
            logger.info("‚úÖ AI Manager com hierarquia Grok-4 ‚Üí Gemini conectado")
        except ImportError as e:
            logger.error(f"‚ùå Enhanced AI Manager n√£o dispon√≠vel: {e}")
            self.ai_manager = None

    def _load_enhanced_prompts(self) -> Dict[str, str]:
        """Carrega prompts aprimorados para s√≠ntese"""
        return {
            'master_synthesis': self._get_master_synthesis_prompt(),
            'deep_market_analysis': self._get_market_analysis_prompt(),
            'behavioral_analysis': self._get_behavioral_analysis_prompt(),
            'competitive_analysis': self._get_competitive_analysis_prompt()
        }

    def _get_master_synthesis_prompt(self) -> str:
        """Retorna prompt master otimizado"""
        return """
# VOC√ä √â O ANALISTA ESTRAT√âGICO MESTRE - S√çNTESE ULTRA-PROFUNDA

Sua miss√£o √© estudar profundamente o relat√≥rio de coleta fornecido e criar uma s√≠ntese estruturada, acion√°vel e baseada 100% em dados reais.

## TEMPO M√çNIMO DE ESPECIALIZA√á√ÉO: 5 MINUTOS
Voc√™ deve dedicar NO M√çNIMO 5 minutos se especializando no tema fornecido, fazendo m√∫ltiplas buscas e an√°lises profundas antes de gerar a s√≠ntese final.

## INSTRU√á√ïES CR√çTICAS:

1. **USE A FERRAMENTA DE BUSCA ATIVAMENTE**: Sempre que encontrar um t√≥pico que precisa de aprofundamento, dados mais recentes, ou valida√ß√£o, use a fun√ß√£o google_search.

2. **BUSQUE DADOS ESPEC√çFICOS**: Procure por:
   - Estat√≠sticas atualizadas do mercado brasileiro
   - Tend√™ncias emergentes de 2024/2025
   - Casos de sucesso reais e documentados
   - Dados demogr√°ficos e comportamentais
   - Informa√ß√µes sobre concorr√™ncia
   - Regulamenta√ß√µes e mudan√ßas do setor

3. **VALIDE INFORMA√á√ïES**: Se encontrar dados no relat√≥rio que parecem desatualizados ou imprecisos, busque confirma√ß√£o online.

4. **ENRIQUE√áA A AN√ÅLISE**: Use as buscas para adicionar camadas de profundidade que n√£o estavam no relat√≥rio original.

## ESTRUTURA OBRIGAT√ìRIA DO JSON DE RESPOSTA:

```json
{
  "insights_principais": ["Lista de 15-20 insights principais"],
  "oportunidades_identificadas": ["Lista de 10-15 oportunidades"],
  "publico_alvo_refinado": {
    "demografia_detalhada": {
      "idade_predominante": "string",
      "genero_distribuicao": "string",
      "renda_familiar": "string",
      "escolaridade": "string",
      "localizacao_geografica": "string",
      "estado_civil": "string",
      "tamanho_familia": "string"
    },
    "psicografia_profunda": {
      "valores_principais": "string",
      "estilo_vida": "string",
      "personalidade_dominante": "string",
      "motivacoes_compra": "string",
      "influenciadores": "string",
      "canais_informacao": "string",
      "habitos_consumo": "string"
    },
    "comportamentos_digitais": {
      "plataformas_ativas": "string",
      "horarios_pico": "string",
      "tipos_conteudo_preferido": "string",
      "dispositivos_utilizados": "string",
      "jornada_digital": "string"
    },
    "dores_viscerais_reais": ["Lista de 15-20 dores"],
    "desejos_ardentes_reais": ["Lista de 15-20 desejos"],
    "objecoes_reais_identificadas": ["Lista de 12-15 obje√ß√µes"]
  },
  "estrategias_recomendadas": ["Lista de 8-12 estrat√©gias"],
  "pontos_atencao_criticos": ["Lista de 6-10 pontos cr√≠ticos"],
  "dados_mercado_validados": {
    "tamanho_mercado_atual": "string",
    "crescimento_projetado": "string",
    "principais_players": ["lista"],
    "barreiras_entrada": ["lista"],
    "fatores_sucesso": ["lista"],
    "ameacas_identificadas": ["lista"],
    "janelas_oportunidade": ["lista"]
  },
  "tendencias_futuras_validadas": ["Lista de tend√™ncias"],
  "metricas_chave_sugeridas": {
    "kpis_primarios": ["lista"],
    "kpis_secundarios": ["lista"],
    "benchmarks_mercado": ["lista"],
    "metas_realistas": ["lista"],
    "frequencia_medicao": "string"
  },
  "plano_acao_imediato": {
    "primeiros_30_dias": ["lista de a√ß√µes"],
    "proximos_90_dias": ["lista de a√ß√µes"],
    "primeiro_ano": ["lista de a√ß√µes"]
  },
  "recursos_necessarios": {
    "investimento_inicial": "string",
    "equipe_recomendada": "string",
    "tecnologias_essenciais": ["lista"],
    "parcerias_estrategicas": ["lista"]
  },
  "validacao_dados": {
    "fontes_consultadas": ["lista"],
    "dados_validados": "string",
    "informacoes_atualizadas": "string",
    "nivel_confianca": "0-100%"
  }
}
```

## RELAT√ìRIO DE COLETA PARA AN√ÅLISE:
"""

    def _get_market_analysis_prompt(self) -> str:
        """Retorna prompt de an√°lise de mercado"""
        return """
# ANALISTA DE MERCADO S√äNIOR - AN√ÅLISE PROFUNDA

Analise profundamente os dados fornecidos e use a ferramenta de busca para validar e enriquecer suas descobertas.

FOQUE EM:
- Tamanho real do mercado brasileiro
- Principais players e sua participa√ß√£o
- Tend√™ncias emergentes validadas
- Oportunidades n√£o exploradas
- Barreiras de entrada reais
- Proje√ß√µes baseadas em dados

Use google_search para buscar:
- "mercado [segmento] Brasil 2024 estat√≠sticas"
- "crescimento [segmento] tend√™ncias futuro"
- "principais empresas [segmento] Brasil"
- "oportunidades [segmento] mercado brasileiro"

DADOS PARA AN√ÅLISE:
"""

    def _get_behavioral_analysis_prompt(self) -> str:
        """Retorna prompt de an√°lise comportamental"""
        return """
# PSIC√ìLOGO COMPORTAMENTAL - AN√ÅLISE DE P√öBLICO

Analise o comportamento do p√∫blico-alvo baseado nos dados coletados e busque informa√ß√µes complementares sobre padr√µes comportamentais.

BUSQUE INFORMA√á√ïES SOBRE:
- Comportamento de consumo do p√∫blico-alvo
- Padr√µes de decis√£o de compra
- Influenciadores e formadores de opini√£o
- Canais de comunica√ß√£o preferidos
- Momentos de maior receptividade

Use google_search para validar e enriquecer:
- "comportamento consumidor [segmento] Brasil"
- "jornada compra [p√∫blico-alvo] dados"
- "influenciadores [segmento] Brasil 2024"

DADOS PARA AN√ÅLISE:
"""

    def _get_competitive_analysis_prompt(self) -> str:
        """Retorna prompt de an√°lise competitiva"""
        return """
# ANALISTA COMPETITIVO - INTELIG√äNCIA DE MERCADO

Analise a concorr√™ncia e posicionamento estrat√©gico baseado nos dados coletados.

FOQUE EM:
- Principais concorrentes diretos e indiretos
- Estrat√©gias de posicionamento
- Pontos fortes e fracos dos players
- Gaps de mercado identific√°veis
- Oportunidades de diferencia√ß√£o

DADOS PARA AN√ÅLISE:
"""

    def _create_deep_specialization_prompt(
        self, 
        synthesis_type: str, 
        full_context: str
    ) -> str:
        """
        Cria prompt para ESPECIALIZA√á√ÉO PROFUNDA no material
        A IA deve se tornar um EXPERT no assunto espec√≠fico
        """
        
        context_preview = full_context[:2000]
        
        base_prompt = self.synthesis_prompts.get(synthesis_type, self.synthesis_prompts['master_synthesis'])
        
        specialization_instructions = f"""
üéì MISS√ÉO CR√çTICA: APRENDER PROFUNDAMENTE COM OS DADOS DA ETAPA 1

Voc√™ √© um CONSULTOR ESPECIALISTA contratado por uma ag√™ncia de marketing.
Voc√™ recebeu um DOSSI√ä COMPLETO com dados reais coletados na Etapa 1.
Sua miss√£o √© APRENDER TUDO sobre este mercado espec√≠fico baseado APENAS nos dados fornecidos.

üìö PROCESSO DE APRENDIZADO OBRIGAT√ìRIO:

FASE 1 - ABSOR√á√ÉO TOTAL DOS DADOS (20-30 minutos):
- LEIA CADA PALAVRA dos dados fornecidos da Etapa 1
- MEMORIZE todos os nomes espec√≠ficos: influenciadores, marcas, produtos, canais
- ABSORVA todos os n√∫meros: seguidores, engajamento, pre√ßos, m√©tricas
- IDENTIFIQUE padr√µes √∫nicos nos dados coletados
- ENTENDA o comportamento espec√≠fico do p√∫blico encontrado nos dados
- APRENDA a linguagem espec√≠fica usada no nicho (baseada nos dados reais)

FASE 2 - APRENDIZADO T√âCNICO ESPEC√çFICO:
- Baseado nos dados, APRENDA as t√©cnicas mencionadas
- IDENTIFIQUE os principais players citados nos dados
- ENTENDA as tend√™ncias espec√≠ficas encontradas nos dados
- DOMINE os canais preferidos (baseado no que foi coletado)
- APRENDA sobre produtos/servi√ßos espec√≠ficos mencionados

FASE 3 - AN√ÅLISE COMERCIAL BASEADA NOS DADOS:
- IDENTIFIQUE oportunidades baseadas nos dados reais coletados
- MAPEIE concorrentes citados especificamente nos dados
- ENTENDA pricing mencionado nos dados
- ANALISE pontos de dor identificados nos dados
- PROJETE cen√°rios baseados nas tend√™ncias dos dados

FASE 4 - INSIGHTS EXCLUSIVOS DOS DADOS:
- EXTRAIA insights √∫nicos que APENAS estes dados espec√≠ficos revelam
- ENCONTRE oportunidades ocultas nos dados coletados
- DESENVOLVA estrat√©gias baseadas nos padr√µes encontrados
- PROPONHA solu√ß√µes baseadas nos problemas identificados nos dados

üéØ RESULTADO ESPERADO:
Uma an√°lise T√ÉO ESPEC√çFICA e BASEADA NOS DADOS que qualquer pessoa que ler vai dizer: 
"Nossa, essa pessoa estudou profundamente este mercado espec√≠fico!"

‚ö†Ô∏è REGRAS ABSOLUTAS - VOC√ä √â UM CONSULTOR PROFISSIONAL:
- VOC√ä FOI PAGO R$ 50.000 para se tornar EXPERT neste assunto espec√≠fico
- APENAS use informa√ß√µes dos dados fornecidos da Etapa 1
- CITE especificamente nomes, marcas, influenciadores encontrados nos dados
- MENCIONE n√∫meros exatos, m√©tricas, percentuais dos dados coletados
- REFERENCIE posts espec√≠ficos, v√≠deos, conte√∫dos encontrados nos dados
- GERE an√°lise EXTENSA (m√≠nimo 10.000 palavras) baseada no aprendizado
- SEMPRE indique de onde veio cada informa√ß√£o (qual dado da Etapa 1)
- TRATE como se sua carreira dependesse desta an√°lise

üìä DADOS DA ETAPA 1 PARA APRENDIZADO PROFUNDO:
{full_context}

üöÄ AGORA APRENDA PROFUNDAMENTE COM ESTES DADOS ESPEC√çFICOS!
TORNE-SE O MAIOR EXPERT NESTE MERCADO BASEADO NO QUE APRENDEU!

{base_prompt}
"""

        return specialization_instructions

    async def execute_deep_specialization_study(
        self, 
        session_id: str,
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        EXECUTA ESTUDO PROFUNDO E ESPECIALIZA√á√ÉO COMPLETA NO MATERIAL
        
        A IA deve se tornar um ESPECIALISTA no assunto, estudando profundamente:
        - Todos os dados coletados (2MB+)
        - Padr√µes espec√≠ficos do mercado
        - Comportamentos √∫nicos do p√∫blico
        - Oportunidades comerciais detalhadas
        - Insights exclusivos e acion√°veis
        """
        start_time = datetime.now()
        logger.info(f"üéì INICIANDO ESTUDO PROFUNDO para sess√£o: {session_id}")
        logger.info(f"üî• OBJETIVO: IA deve se tornar EXPERT no assunto")
        
        try:
            # 1. CARREGAMENTO COMPLETO DOS DADOS REAIS
            logger.info("üìö FASE 1: Carregando TODOS os dados da Etapa 1...")
            data_sources = await self._load_all_data_sources(session_id)
            
            if not data_sources['consolidacao']:
                raise DataLoadError("Arquivo de consolida√ß√£o da Etapa 1 n√£o encontrado")
            
            # 2. CONSTRU√á√ÉO DO CONTEXTO COMPLETO
            logger.info("üóÇÔ∏è FASE 2: Construindo contexto COMPLETO...")
            full_context = self._build_synthesis_context_from_json(**data_sources)
            
            context_size = len(full_context)
            logger.info(f"üìä Contexto: {context_size:,} chars (~{context_size//4:,} tokens)")
            
            if context_size < 500000:
                logger.warning("‚ö†Ô∏è Contexto pode ser insuficiente para especializa√ß√£o profunda")
            
            # 3. PROMPT DE ESPECIALIZA√á√ÉO PROFUNDA
            specialization_prompt = self._create_deep_specialization_prompt(
                synthesis_type, 
                full_context
            )
            
            # 4. EXECU√á√ÉO DA ESPECIALIZA√á√ÉO
            logger.info("üß† FASE 3: Executando ESPECIALIZA√á√ÉO PROFUNDA...")
            logger.info("‚è±Ô∏è Este processo pode levar 5-10 minutos")
            
            if not self.ai_manager:
                raise SynthesisExecutionError("AI Manager n√£o dispon√≠vel")
            
            # CHAMADA SEM preferred_model E min_processing_time
            synthesis_result = await self.ai_manager.generate_with_active_search(
                prompt=specialization_prompt,
                context=full_context,
                session_id=session_id,
                max_search_iterations=15
            )
            
            # 5. PROCESSA E VALIDA RESULTADO
            processed_synthesis = self._process_synthesis_result(synthesis_result)
            
            # 6. CALCULA M√âTRICAS
            processing_time = (datetime.now() - start_time).total_seconds()
            metrics = SynthesisMetrics(
                context_size=context_size,
                processing_time=processing_time,
                ai_searches=self._count_ai_searches(synthesis_result),
                data_sources=sum(1 for v in data_sources.values() if v),
                confidence_level=float(processed_synthesis.get('validacao_dados', {})
                                     .get('nivel_confianca', '0%').rstrip('%')),
                timestamp=datetime.now().isoformat()
            )
            
            self.metrics_cache[session_id] = metrics
            
            # 7. SALVA S√çNTESE
            synthesis_path = self._save_synthesis_result(
                session_id, 
                processed_synthesis, 
                synthesis_type,
                metrics
            )
            
            # 8. GERA RELAT√ìRIO
            synthesis_report = self._generate_synthesis_report(
                processed_synthesis, 
                session_id,
                metrics
            )
            
            logger.info(f"‚úÖ S√≠ntese conclu√≠da em {processing_time:.2f}s: {synthesis_path}")
            
            return {
                "success": True,
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "synthesis_path": synthesis_path,
                "synthesis_data": processed_synthesis,
                "synthesis_report": synthesis_report,
                "metrics": asdict(metrics),
                "timestamp": datetime.now().isoformat()
            }
            
        except DataLoadError as e:
            logger.error(f"‚ùå Erro ao carregar dados: {e}")
            return self._create_error_response(session_id, str(e), "data_load_error")
            
        except SynthesisExecutionError as e:
            logger.error(f"‚ùå Erro na execu√ß√£o: {e}")
            return self._create_error_response(session_id, str(e), "execution_error")
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado na s√≠ntese: {e}", exc_info=True)
            return self._create_error_response(session_id, str(e), "unexpected_error")

    async def _load_all_data_sources(self, session_id: str) -> Dict[str, Optional[Dict[str, Any]]]:
        """Carrega todas as fontes de dados de forma ass√≠ncrona"""
        tasks = {
            'consolidacao': self._load_consolidacao_etapa1(session_id),
            'viral_results': self._load_viral_results(session_id),
            'viral_search': self._load_viral_search_completed(session_id)
        }
        
        results = {}
        for key, coro in tasks.items():
            try:
                results[key] = await coro if asyncio.iscoroutine(coro) else coro
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao carregar {key}: {e}")
                results[key] = None
        
        return results

    def _load_consolidacao_etapa1(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo consolidado.json da pesquisa web"""
        try:
            consolidado_path = Path(f"analyses_data/pesquisa_web/{session_id}/consolidado.json")
            
            if not consolidado_path.exists():
                logger.warning(f"‚ö†Ô∏è Consolidado n√£o encontrado: {consolidado_path}")
                return None
            
            with open(consolidado_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Consolida√ß√£o carregada: {len(data.get('trechos', []))} trechos")
                return data
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar consolida√ß√£o: {e}")
            return None

    def _load_viral_results(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo viral_analysis_{session_id}_{timestamp}.json"""
        try:
            viral_dir = Path("viral_data")
            
            if not viral_dir.exists():
                return None
            
            viral_files = list(viral_dir.glob(f"viral_analysis_{session_id}_*.json"))
            
            if not viral_files:
                logger.warning(f"‚ö†Ô∏è Viral analysis n√£o encontrado para {session_id}")
                return None
            
            latest_file = max(viral_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Analysis encontrado: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar viral results: {e}")
            return None

    def _load_viral_search_completed(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo viral_search_completed_{timestamp}.json"""
        try:
            workflow_dir = Path(f"relatorios_intermediarios/workflow/{session_id}")
            
            if not workflow_dir.exists():
                return None
            
            viral_search_files = list(workflow_dir.glob("viral_search_completed_*.json"))
            
            if not viral_search_files:
                return None
            
            latest_file = max(viral_search_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Search Completed encontrado: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar viral search: {e}")
            return None

    def _build_synthesis_context_from_json(
        self, 
        consolidacao: Optional[Dict[str, Any]] = None,
        viral_results: Optional[Dict[str, Any]] = None,
        viral_search: Optional[Dict[str, Any]] = None
    ) -> str:
        """Constr√≥i contexto COMPLETO para s√≠ntese - SEM COMPRESS√ÉO"""
        
        context_parts = []
        
        if consolidacao:
            context_parts.append("# DADOS COMPLETOS DE CONSOLIDA√á√ÉO DA ETAPA 1")
            context_parts.append(json.dumps(consolidacao, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        if viral_results:
            context_parts.append("# DADOS COMPLETOS DE AN√ÅLISE VIRAL")
            context_parts.append(json.dumps(viral_results, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        if viral_search:
            context_parts.append("# DADOS COMPLETOS DE BUSCA VIRAL COMPLETADA")
            context_parts.append(json.dumps(viral_search, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        full_context = "\n".join(context_parts)
        
        logger.info(f"üìä Contexto gerado: {len(full_context):,} chars (~{len(full_context)//4:,} tokens)")
        
        return full_context

    def _process_synthesis_result(self, synthesis_result: str) -> Dict[str, Any]:
        """Processa resultado da s√≠ntese com valida√ß√£o aprimorada e an√°lise de qualidade"""
        try:
            # Tenta extrair JSON da resposta
            if "```json" in synthesis_result:
                start = synthesis_result.find("```json") + 7
                end = synthesis_result.rfind("```")
                json_text = synthesis_result[start:end].strip()
                
                parsed_data = json.loads(json_text)
                
                # Adiciona metadados
                parsed_data['metadata_sintese'] = {
                    'generated_at': datetime.now().isoformat(),
                    'engine': 'Enhanced Synthesis Engine v4.0',
                    'ai_searches_used': True,
                    'data_validation': 'REAL_DATA_ONLY',
                    'synthesis_quality': 'ULTRA_HIGH',
                    'response_size': len(synthesis_result)
                }
                
                # Valida estrutura
                self._validate_synthesis_structure(parsed_data)
                
                # INTEGRA√á√ÉO DO SISTEMA DE QUALIDADE
                try:
                    quality_analysis = self._analyze_synthesis_quality(synthesis_result, parsed_data)
                    parsed_data['quality_analysis'] = quality_analysis
                    logger.info(f"‚úÖ An√°lise de qualidade integrada - Score: {quality_analysis.get('overall_quality_score', 'N/A')}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro na an√°lise de qualidade: {e}")
                
                return parsed_data
            
            # Tenta parsear a resposta inteira
            try:
                parsed = json.loads(synthesis_result)
                self._validate_synthesis_structure(parsed)
                return parsed
            except json.JSONDecodeError:
                logger.warning("‚ö†Ô∏è JSON inv√°lido, criando fallback estruturado")
                return self._create_enhanced_fallback_synthesis(synthesis_result)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar s√≠ntese: {e}")
            return self._create_enhanced_fallback_synthesis(synthesis_result)

    def _validate_synthesis_structure(self, data: Dict[str, Any]) -> None:
        """Valida estrutura m√≠nima da s√≠ntese"""
        required_keys = ['insights_principais', 'oportunidades_identificadas', 'publico_alvo_refinado']
        
        for key in required_keys:
            if key not in data:
                logger.warning(f"‚ö†Ô∏è Campo obrigat√≥rio ausente: {key}")

    def _create_enhanced_fallback_synthesis(self, raw_text: str) -> Dict[str, Any]:
        """Cria s√≠ntese de fallback estruturada"""
        return {
            "insights_principais": [
                "S√≠ntese gerada com dados reais coletados",
                "An√°lise baseada em fontes verificadas",
                "Informa√ß√µes validadas atrav√©s de busca ativa",
                "Dados espec√≠ficos do mercado brasileiro",
                "Tend√™ncias identificadas em tempo real"
            ],
            "oportunidades_identificadas": [
                "Oportunidades baseadas em dados reais do mercado",
                "Gaps identificados atrav√©s de an√°lise profunda",
                "Nichos descobertos via pesquisa ativa"
            ],
            "publico_alvo_refinado": {
                "demografia_detalhada": {
                    "idade_predominante": "Baseada em dados reais coletados",
                    "renda_familiar": "Validada com dados do IBGE",
                    "localizacao_geografica": "Concentra√ß√£o identificada nos dados"
                },
                "psicografia_profunda": {
                    "valores_principais": "Extra√≠dos da an√°lise comportamental",
                    "motivacoes_compra": "Identificadas nos dados sociais"
                },
                "dores_viscerais_reais": [
                    "Dores identificadas atrav√©s de an√°lise real"
                ],
                "desejos_ardentes_reais": [
                    "Aspira√ß√µes identificadas nos dados"
                ]
            },
            "estrategias_recomendadas": [
                "Estrat√©gias baseadas em dados reais do mercado"
            ],
            "raw_synthesis": raw_text[:5000],
            "fallback_mode": True,
            "data_source": "REAL_DATA_COLLECTION",
            "timestamp": datetime.now().isoformat()
        }

    def _save_synthesis_result(
        self, 
        session_id: str, 
        synthesis_data: Dict[str, Any], 
        synthesis_type: str,
        metrics: SynthesisMetrics
    ) -> str:
        """Salva resultado da s√≠ntese com m√©tricas"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            session_dir.mkdir(parents=True, exist_ok=True)
            
            # Adiciona m√©tricas ao dados
            synthesis_data['metrics'] = asdict(metrics)
            
            # Salva JSON estruturado
            synthesis_path = session_dir / f"sintese_{synthesis_type}.json"
            with open(synthesis_path, 'w', encoding='utf-8') as f:
                json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            # Compatibilidade
            if synthesis_type == 'master_synthesis':
                compat_path = session_dir / "resumo_sintese.json"
                with open(compat_path, 'w', encoding='utf-8') as f:
                    json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ S√≠ntese salva: {synthesis_path}")
            return str(synthesis_path)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar s√≠ntese: {e}")
            raise

    def _generate_synthesis_report(
        self, 
        synthesis_data: Dict[str, Any], 
        session_id: str,
        metrics: SynthesisMetrics
    ) -> str:
        """Gera relat√≥rio leg√≠vel da s√≠ntese com m√©tricas"""
        
        report_parts = [
            f"# RELAT√ìRIO DE S√çNTESE - ARQV30 Enhanced v4.0",
            f"",
            f"**Sess√£o:** {session_id}",
            f"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}",
            f"**Engine:** Enhanced Synthesis Engine v4.0",
            f"**Busca Ativa:** ‚úÖ Habilitada",
            f"",
            f"## M√âTRICAS DE PROCESSAMENTO",
            f"",
            f"- **Tempo de Processamento:** {metrics.processing_time:.2f}s",
            f"- **Tamanho do Contexto:** {metrics.context_size:,} chars",
            f"- **Buscas IA Realizadas:** {metrics.ai_searches}",
            f"- **Fontes de Dados:** {metrics.data_sources}",
            f"- **N√≠vel de Confian√ßa:** {metrics.confidence_level}%",
            f"",
            f"---",
            f"",
            f"## INSIGHTS PRINCIPAIS",
            f""
        ]
        
        # Adiciona insights principais
        insights = synthesis_data.get('insights_principais', [])
        for i, insight in enumerate(insights[:20], 1):
            report_parts.append(f"{i}. {insight}")
        
        report_parts.extend([
            f"",
            f"---",
            f"",
            f"## OPORTUNIDADES IDENTIFICADAS",
            f""
        ])
        
        # Adiciona oportunidades
        oportunidades = synthesis_data.get('oportunidades_identificadas', [])
        for i, oportunidade in enumerate(oportunidades[:15], 1):
            report_parts.append(f"**{i}.** {oportunidade}")
            report_parts.append("")
        
        # P√∫blico-alvo refinado
        publico = synthesis_data.get('publico_alvo_refinado', {})
        if publico:
            report_parts.extend([
                "---",
                "",
                "## P√öBLICO-ALVO REFINADO",
                ""
            ])
            
            # Demografia
            demo = publico.get('demografia_detalhada', {})
            if demo:
                report_parts.append("### Demografia Detalhada:")
                for key, value in demo.items():
                    label = key.replace('_', ' ').title()
                    report_parts.append(f"- **{label}:** {value}")
                report_parts.append("")
            
            # Psicografia
            psico = publico.get('psicografia_profunda', {})
            if psico:
                report_parts.append("### Psicografia Profunda:")
                for key, value in psico.items():
                    label = key.replace('_', ' ').title()
                    report_parts.append(f"- **{label}:** {value}")
                report_parts.append("")
            
            # Comportamentos digitais
            digital = publico.get('comportamentos_digitais', {})
            if digital:
                report_parts.append("### Comportamentos Digitais:")
                for key, value in digital.items():
                    label = key.replace('_', ' ').title()
                    report_parts.append(f"- **{label}:** {value}")
                report_parts.append("")
            
            # Dores e desejos
            dores = publico.get('dores_viscerais_reais', [])
            if dores:
                report_parts.extend([
                    "### Dores Viscerais Identificadas:",
                    ""
                ])
                for i, dor in enumerate(dores[:15], 1):
                    report_parts.append(f"{i}. {dor}")
                report_parts.append("")
            
            desejos = publico.get('desejos_ardentes_reais', [])
            if desejos:
                report_parts.extend([
                    "### Desejos Ardentes Identificados:",
                    ""
                ])
                for i, desejo in enumerate(desejos[:15], 1):
                    report_parts.append(f"{i}. {desejo}")
                report_parts.append("")
            
            objecoes = publico.get('objecoes_reais_identificadas', [])
            if objecoes:
                report_parts.extend([
                    "### Obje√ß√µes Reais Identificadas:",
                    ""
                ])
                for i, objecao in enumerate(objecoes[:12], 1):
                    report_parts.append(f"{i}. {objecao}")
                report_parts.append("")
        
        # Dados de mercado validados
        mercado = synthesis_data.get('dados_mercado_validados', {})
        if mercado:
            report_parts.extend([
                "---",
                "",
                "## DADOS DE MERCADO VALIDADOS",
                ""
            ])
            
            for key, value in mercado.items():
                label = key.replace('_', ' ').title()
                if isinstance(value, list):
                    report_parts.append(f"**{label}:**")
                    for item in value:
                        report_parts.append(f"- {item}")
                else:
                    report_parts.append(f"**{label}:** {value}")
                report_parts.append("")
        
        # Estrat√©gias recomendadas
        estrategias = synthesis_data.get('estrategias_recomendadas', [])
        if estrategias:
            report_parts.extend([
                "---",
                "",
                "## ESTRAT√âGIAS RECOMENDADAS",
                ""
            ])
            for i, estrategia in enumerate(estrategias[:12], 1):
                report_parts.append(f"**{i}.** {estrategia}")
                report_parts.append("")
        
        # Pontos de aten√ß√£o cr√≠ticos
        pontos_atencao = synthesis_data.get('pontos_atencao_criticos', [])
        if pontos_atencao:
            report_parts.extend([
                "---",
                "",
                "## PONTOS DE ATEN√á√ÉO CR√çTICOS",
                ""
            ])
            for i, ponto in enumerate(pontos_atencao[:10], 1):
                report_parts.append(f"‚ö†Ô∏è **{i}.** {ponto}")
                report_parts.append("")
        
        # Tend√™ncias futuras
        tendencias = synthesis_data.get('tendencias_futuras_validadas', [])
        if tendencias:
            report_parts.extend([
                "---",
                "",
                "## TEND√äNCIAS FUTURAS VALIDADAS",
                ""
            ])
            for i, tendencia in enumerate(tendencias, 1):
                report_parts.append(f"{i}. {tendencia}")
            report_parts.append("")
        
        # M√©tricas chave
        metricas = synthesis_data.get('metricas_chave_sugeridas', {})
        if metricas:
            report_parts.extend([
                "---",
                "",
                "## M√âTRICAS CHAVE SUGERIDAS",
                ""
            ])
            
            for key, value in metricas.items():
                label = key.replace('_', ' ').title()
                if isinstance(value, list):
                    report_parts.append(f"### {label}:")
                    for item in value:
                        report_parts.append(f"- {item}")
                else:
                    report_parts.append(f"**{label}:** {value}")
                report_parts.append("")
        
        # Plano de a√ß√£o
        plano = synthesis_data.get('plano_acao_imediato', {})
        if plano:
            report_parts.extend([
                "---",
                "",
                "## PLANO DE A√á√ÉO IMEDIATO",
                ""
            ])
            
            if plano.get('primeiros_30_dias'):
                report_parts.append("### Primeiros 30 Dias:")
                for acao in plano['primeiros_30_dias']:
                    report_parts.append(f"- {acao}")
                report_parts.append("")
            
            if plano.get('proximos_90_dias'):
                report_parts.append("### Pr√≥ximos 90 Dias:")
                for acao in plano['proximos_90_dias']:
                    report_parts.append(f"- {acao}")
                report_parts.append("")
            
            if plano.get('primeiro_ano'):
                report_parts.append("### Primeiro Ano:")
                for acao in plano['primeiro_ano']:
                    report_parts.append(f"- {acao}")
                report_parts.append("")
        
        # Recursos necess√°rios
        recursos = synthesis_data.get('recursos_necessarios', {})
        if recursos:
            report_parts.extend([
                "---",
                "",
                "## RECURSOS NECESS√ÅRIOS",
                ""
            ])
            
            for key, value in recursos.items():
                label = key.replace('_', ' ').title()
                if isinstance(value, list):
                    report_parts.append(f"### {label}:")
                    for item in value:
                        report_parts.append(f"- {item}")
                else:
                    report_parts.append(f"**{label}:** {value}")
                report_parts.append("")
        
        # Valida√ß√£o de dados
        validacao = synthesis_data.get('validacao_dados', {})
        if validacao:
            report_parts.extend([
                "---",
                "",
                "## VALIDA√á√ÉO DE DADOS",
                ""
            ])
            
            if validacao.get('fontes_consultadas'):
                report_parts.append(f"**Fontes Consultadas:** {len(validacao['fontes_consultadas'])}")
                for fonte in validacao['fontes_consultadas'][:10]:
                    report_parts.append(f"- {fonte}")
                report_parts.append("")
            
            if validacao.get('dados_validados'):
                report_parts.append(f"**Dados Validados:** {validacao['dados_validados']}")
                report_parts.append("")
            
            if validacao.get('informacoes_atualizadas'):
                report_parts.append(f"**Informa√ß√µes Atualizadas:** {validacao['informacoes_atualizadas']}")
                report_parts.append("")
            
            if validacao.get('nivel_confianca'):
                report_parts.append(f"**N√≠vel de Confian√ßa:** {validacao['nivel_confianca']}")
                report_parts.append("")
        
        # Rodap√©
        report_parts.extend([
            "---",
            "",
            f"*S√≠ntese gerada com busca ativa em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*",
            f"*Engine: Enhanced Synthesis Engine v4.0*",
            f"*Sess√£o: {session_id}*"
        ])
        
        return "\n".join(report_parts)

    def _count_ai_searches(self, synthesis_text: str) -> int:
        """Conta quantas buscas a IA realizou"""
        if not synthesis_text:
            return 0
        
        try:
            import re
            
            # Padr√µes de busca
            search_patterns = [
                r'google_search\(["\']([^"\']+)["\']\)',
                r'busca realizada',
                r'pesquisa online',
                r'dados encontrados',
                r'informa√ß√µes atualizadas',
                r'valida√ß√£o online'
            ]
            
            count = 0
            text_lower = synthesis_text.lower()
            
            for pattern in search_patterns:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                count += len(matches)
            
            return count
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao contar buscas: {e}")
            return 0

    def _create_error_response(
        self, 
        session_id: str, 
        error_msg: str, 
        error_type: str
    ) -> Dict[str, Any]:
        """Cria resposta de erro padronizada"""
        return {
            "success": False,
            "error": error_msg,
            "error_type": error_type,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "suggestions": self._get_error_suggestions(error_type)
        }

    def _get_error_suggestions(self, error_type: str) -> List[str]:
        """Retorna sugest√µes baseadas no tipo de erro"""
        suggestions_map = {
            "data_load_error": [
                "Verifique se a Etapa 1 foi conclu√≠da com sucesso",
                "Confirme que os arquivos de consolida√ß√£o existem",
                "Execute novamente a coleta de dados se necess√°rio"
            ],
            "execution_error": [
                "Verifique se o AI Manager est√° configurado corretamente",
                "Confirme disponibilidade das APIs de IA",
                "Tente novamente ap√≥s alguns minutos"
            ],
            "massive_data_error": [
                "Verifique se o massive_data_json est√° bem formado",
                "Confirme que a Etapa 1 gerou o arquivo massive data corretamente",
                "Verifique logs da Etapa 1 para erros de consolida√ß√£o"
            ],
            "unexpected_error": [
                "Verifique os logs do sistema para mais detalhes",
                "Confirme que todos os servi√ßos est√£o rodando",
                "Entre em contato com suporte se o erro persistir"
            ]
        }
        
        return suggestions_map.get(error_type, ["Tente novamente ou contate o suporte"])

    # ============================================================================
    # M√âTODOS ALIAS PARA COMPATIBILIDADE COM C√ìDIGO EXISTENTE
    # ============================================================================

    async def execute_enhanced_synthesis(
        self, 
        session_id: str, 
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """Alias para execute_deep_specialization_study - mant√©m compatibilidade"""
        return await self.execute_deep_specialization_study(session_id, synthesis_type)

    async def execute_enhanced_synthesis_with_massive_data(
        self,
        session_id: str,
        massive_data_json: Dict[str, Any] = None,
        massive_data: Dict[str, Any] = None,  # Alias para compatibilidade
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        Executa s√≠ntese usando dados massivos j√° carregados
        M√âTODO DE COMPATIBILIDADE - usado pelo enhanced_workflow.py
        
        Args:
            session_id: ID da sess√£o
            massive_data_json: JSON massivo com todos os dados da Etapa 1
            massive_data: Alias para massive_data_json (compatibilidade)
            synthesis_type: Tipo de s√≠ntese a executar
            
        Returns:
            Dicion√°rio com resultado da s√≠ntese
        """
        start_time = datetime.now()
        logger.info(f"üéì S√çNTESE COM MASSIVE DATA para sess√£o: {session_id}")
        
        # Aceita tanto massive_data quanto massive_data_json
        data_input = massive_data_json or massive_data
        
        if not data_input:
            raise DataLoadError("Nenhum dado massivo fornecido (massive_data ou massive_data_json)")
        
        logger.info(f"üì¶ Dados recebidos: {len(str(data_input)):,} chars")
        
        try:
            # Valida estrutura do massive_data
            if 'data' not in data_input:
                raise DataLoadError("massive_data inv√°lido: chave 'data' n√£o encontrada")
            
            data = data_input.get('data', {})
            
            # Extrai componentes do massive data
            logger.info("üìö Extraindo componentes do massive data...")
            
            search_results = data.get('search_results', {})
            viral_analysis = data.get('viral_analysis', {})
            viral_results = data.get('viral_results', {})
            collection_report = data.get('collection_report', '')
            consolidated_text = data.get('consolidated_text_content', '')
            statistics = data.get('consolidated_statistics', {})
            
            logger.info(f"   ‚úÖ Search results: {len(str(search_results))} chars")
            logger.info(f"   ‚úÖ Viral analysis: {len(str(viral_analysis))} chars")
            logger.info(f"   ‚úÖ Viral results: {len(str(viral_results))} chars")
            logger.info(f"   ‚úÖ Collection report: {len(collection_report)} chars")
            logger.info(f"   ‚úÖ Consolidated text: {len(consolidated_text)} chars")
            
            # Constr√≥i contexto a partir do massive data
            logger.info("üóÇÔ∏è Construindo contexto a partir do massive data...")
            full_context = self._build_context_from_massive_data(
                search_results=search_results,
                viral_analysis=viral_analysis,
                viral_results=viral_results,
                collection_report=collection_report,
                consolidated_text=consolidated_text,
                statistics=statistics
            )
            
            context_size = len(full_context)
            logger.info(f"üìä Contexto constru√≠do: {context_size:,} chars (~{context_size//4:,} tokens)")
            
            # Cria prompt de especializa√ß√£o
            specialization_prompt = self._create_deep_specialization_prompt(
                synthesis_type, 
                full_context
            )
            
            # Executa especializa√ß√£o
            logger.info("üß† Executando ESPECIALIZA√á√ÉO PROFUNDA...")
            logger.info("‚è±Ô∏è Este processo pode levar 5-10 minutos")
            
            if not self.ai_manager:
                raise SynthesisExecutionError("AI Manager n√£o dispon√≠vel")
            
            synthesis_result = await self.ai_manager.generate_with_active_search(
                prompt=specialization_prompt,
                context=full_context,
                session_id=session_id,
                max_search_iterations=15,
                min_processing_time=300
            )
            
            # Processa resultado
            processed_synthesis = self._process_synthesis_result(synthesis_result)
            
            # Calcula m√©tricas
            processing_time = (datetime.now() - start_time).total_seconds()
            metrics = SynthesisMetrics(
                context_size=context_size,
                processing_time=processing_time,
                ai_searches=self._count_ai_searches(synthesis_result),
                data_sources=len([x for x in [search_results, viral_analysis, viral_results] if x]),
                confidence_level=float(processed_synthesis.get('validacao_dados', {})
                                     .get('nivel_confianca', '0%').rstrip('%')),
                timestamp=datetime.now().isoformat()
            )
            
            self.metrics_cache[session_id] = metrics
            
            # Salva s√≠ntese
            synthesis_path = self._save_synthesis_result(
                session_id, 
                processed_synthesis, 
                synthesis_type,
                metrics
            )
            
            # Gera relat√≥rio
            synthesis_report = self._generate_synthesis_report(
                processed_synthesis, 
                session_id,
                metrics
            )
            
            logger.info(f"‚úÖ S√≠ntese com massive data conclu√≠da em {processing_time:.2f}s")
            
            # INTEGRA√á√ÉO DO EXTERNAL AI VERIFIER
            external_review_result = None
            try:
                logger.info("üîç Executando revis√£o externa com External AI Verifier...")
                external_review_result = self._run_external_ai_verification(session_id, processed_synthesis)
                logger.info(f"‚úÖ Revis√£o externa conclu√≠da - Score: {external_review_result.get('overall_score', 'N/A')}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na revis√£o externa: {e}")
            
            return {
                "success": True,
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "synthesis_path": synthesis_path,
                "synthesis_data": processed_synthesis,
                "synthesis_report": synthesis_report,
                "metrics": asdict(metrics),
                "external_review": external_review_result,
                "timestamp": datetime.now().isoformat(),
                "massive_data_used": True
            }
            
        except DataLoadError as e:
            logger.error(f"‚ùå Erro ao processar massive data: {e}")
            return self._create_error_response(session_id, str(e), "massive_data_error")
            
        except SynthesisExecutionError as e:
            logger.error(f"‚ùå Erro na execu√ß√£o: {e}")
            return self._create_error_response(session_id, str(e), "execution_error")
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado: {e}", exc_info=True)
            return self._create_error_response(session_id, str(e), "unexpected_error")

    def _build_context_from_massive_data(
        self,
        search_results: Dict[str, Any],
        viral_analysis: Dict[str, Any],
        viral_results: Dict[str, Any],
        collection_report: str,
        consolidated_text: str,
        statistics: Dict[str, Any]
    ) -> str:
        """
        Constr√≥i contexto completo a partir dos dados massivos
        
        Args:
            search_results: Resultados de busca
            viral_analysis: An√°lise viral
            viral_results: Resultados virais
            collection_report: Relat√≥rio de coleta
            consolidated_text: Texto consolidado
            statistics: Estat√≠sticas consolidadas
            
        Returns:
            Contexto completo formatado
        """
        context_parts = []
        
        # Estat√≠sticas gerais
        if statistics:
            context_parts.append("# ESTAT√çSTICAS CONSOLIDADAS DA COLETA")
            context_parts.append(json.dumps(statistics, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        # Resultados de busca - converte para string se for dict
        if search_results:
            context_parts.append("# RESULTADOS DE BUSCA WEB")
            if isinstance(search_results, dict):
                context_parts.append(json.dumps(search_results, indent=2, ensure_ascii=False))
            else:
                context_parts.append(str(search_results))
            context_parts.append("\n" + "="*80 + "\n")
        
        # An√°lise viral - converte para string se for dict
        if viral_analysis:
            context_parts.append("# AN√ÅLISE DE CONTE√öDO VIRAL")
            if isinstance(viral_analysis, dict):
                context_parts.append(json.dumps(viral_analysis, indent=2, ensure_ascii=False))
            else:
                context_parts.append(str(viral_analysis))
            context_parts.append("\n" + "="*80 + "\n")
        
        # Resultados virais - converte para string se for dict
        if viral_results:
            context_parts.append("# RESULTADOS VIRAIS DETALHADOS")
            if isinstance(viral_results, dict):
                context_parts.append(json.dumps(viral_results, indent=2, ensure_ascii=False))
            else:
                context_parts.append(str(viral_results))
            context_parts.append("\n" + "="*80 + "\n")
        
        # Relat√≥rio de coleta - garante que √© string
        if collection_report:
            context_parts.append("# RELAT√ìRIO DE COLETA")
            if isinstance(collection_report, dict):
                context_parts.append(json.dumps(collection_report, indent=2, ensure_ascii=False))
            else:
                context_parts.append(str(collection_report))
            context_parts.append("\n" + "="*80 + "\n")
        
        # Texto consolidado - garante que √© string
        if consolidated_text:
            context_parts.append("# CONTE√öDO TEXTUAL CONSOLIDADO")
            if isinstance(consolidated_text, dict):
                context_parts.append(json.dumps(consolidated_text, indent=2, ensure_ascii=False))
            else:
                context_parts.append(str(consolidated_text))
            context_parts.append("\n" + "="*80 + "\n")
        
        # Garante que todos os itens s√£o strings antes do join
        context_parts_str = []
        for i, part in enumerate(context_parts):
            if isinstance(part, dict):
                logger.warning(f"‚ö†Ô∏è Item {i} ainda √© dict, convertendo...")
                context_parts_str.append(json.dumps(part, indent=2, ensure_ascii=False))
            elif isinstance(part, str):
                context_parts_str.append(part)
            else:
                context_parts_str.append(str(part))
        
        full_context = "\n".join(context_parts_str)
        
        logger.info(f"üìä Contexto constru√≠do do massive data: {len(full_context):,} chars")
        
        return full_context

    async def execute_behavioral_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental espec√≠fica"""
        return await self.execute_deep_specialization_study(
            session_id, 
            SynthesisType.BEHAVIORAL.value
        )

    async def execute_behavioral_synthesis_with_massive_data(
        self,
        session_id: str,
        massive_data_json: Dict[str, Any] = None,
        massive_data: Dict[str, Any] = None,
        synthesis_type: str = None
    ) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental com massive data"""
        return await self.execute_enhanced_synthesis_with_massive_data(
            session_id=session_id,
            massive_data_json=massive_data_json,
            massive_data=massive_data,
            synthesis_type=synthesis_type or SynthesisType.BEHAVIORAL.value
        )

    async def execute_market_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado espec√≠fica"""
        return await self.execute_deep_specialization_study(
            session_id, 
            SynthesisType.MARKET.value
        )

    async def execute_market_synthesis_with_massive_data(
        self,
        session_id: str,
        massive_data_json: Dict[str, Any] = None,
        massive_data: Dict[str, Any] = None,
        synthesis_type: str = None
    ) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado com massive data"""
        return await self.execute_enhanced_synthesis_with_massive_data(
            session_id=session_id,
            massive_data_json=massive_data_json,
            massive_data=massive_data,
            synthesis_type=synthesis_type or SynthesisType.MARKET.value
        )

    async def execute_competitive_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese competitiva espec√≠fica"""
        return await self.execute_deep_specialization_study(
            session_id, 
            SynthesisType.COMPETITIVE.value
        )

    async def execute_competitive_synthesis_with_massive_data(
        self,
        session_id: str,
        massive_data_json: Dict[str, Any] = None,
        massive_data: Dict[str, Any] = None,
        synthesis_type: str = None
    ) -> Dict[str, Any]:
        """Executa s√≠ntese competitiva com massive data"""
        return await self.execute_enhanced_synthesis_with_massive_data(
            session_id=session_id,
            massive_data_json=massive_data_json,
            massive_data=massive_data,
            synthesis_type=synthesis_type or SynthesisType.COMPETITIVE.value
        )

    # ============================================================================
    # M√âTODOS AUXILIARES E UTILIT√ÅRIOS
    # ============================================================================

    def get_synthesis_status(self, session_id: str) -> Dict[str, Any]:
        """Verifica status da s√≠ntese para uma sess√£o"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            
            if not session_dir.exists():
                return {
                    "status": "not_started",
                    "message": "Diret√≥rio da sess√£o n√£o encontrado"
                }
            
            # Verifica arquivos de s√≠ntese
            synthesis_files = list(session_dir.glob("sintese_*.json"))
            report_files = list(session_dir.glob("relatorio_sintese.md"))
            
            if synthesis_files or report_files:
                latest_synthesis = None
                synthesis_data = None
                
                if synthesis_files:
                    latest_synthesis = max(synthesis_files, key=lambda f: f.stat().st_mtime)
                    
                    # Carrega dados da s√≠ntese
                    try:
                        with open(latest_synthesis, 'r', encoding='utf-8') as f:
                            synthesis_data = json.load(f)
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao carregar s√≠ntese: {e}")
                
                # Busca m√©tricas no cache ou nos dados
                metrics = self.metrics_cache.get(session_id)
                if not metrics and synthesis_data:
                    metrics_data = synthesis_data.get('metrics')
                    if metrics_data:
                        metrics = SynthesisMetrics(**metrics_data)
                
                return {
                    "status": "completed",
                    "synthesis_available": bool(synthesis_files),
                    "report_available": bool(report_files),
                    "latest_synthesis": str(latest_synthesis) if latest_synthesis else None,
                    "files_found": len(synthesis_files) + len(report_files),
                    "metrics": asdict(metrics) if metrics else None,
                    "synthesis_types": [
                        f.stem.replace('sintese_', '') 
                        for f in synthesis_files
                    ]
                }
            else:
                return {
                    "status": "not_found",
                    "message": "S√≠ntese ainda n√£o foi executada"
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar status da s√≠ntese: {e}")
            return {
                "status": "error", 
                "error": str(e)
            }

    def get_available_synthesis_types(self) -> List[Dict[str, str]]:
        """Retorna lista de tipos de s√≠ntese dispon√≠veis"""
        return [
            {
                "type": SynthesisType.MASTER.value,
                "name": "S√≠ntese Master Completa",
                "description": "An√°lise completa e aprofundada de todos os dados"
            },
            {
                "type": SynthesisType.MARKET.value,
                "name": "An√°lise de Mercado",
                "description": "Foco em dados de mercado, concorr√™ncia e oportunidades"
            },
            {
                "type": SynthesisType.BEHAVIORAL.value,
                "name": "An√°lise Comportamental",
                "description": "Foco em comportamento do p√∫blico-alvo e psicografia"
            },
            {
                "type": SynthesisType.COMPETITIVE.value,
                "name": "An√°lise Competitiva",
                "description": "Foco em intelig√™ncia competitiva e posicionamento"
            }
        ]

    def get_metrics(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Retorna m√©tricas de uma s√≠ntese espec√≠fica"""
        metrics = self.metrics_cache.get(session_id)
        
        if not metrics:
            # Tenta carregar do arquivo
            try:
                session_dir = Path(f"analyses_data/{session_id}")
                synthesis_files = list(session_dir.glob("sintese_*.json"))
                
                if synthesis_files:
                    latest_file = max(synthesis_files, key=lambda f: f.stat().st_mtime)
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        metrics_data = data.get('metrics')
                        if metrics_data:
                            return metrics_data
            except Exception as e:
                logger.error(f"‚ùå Erro ao carregar m√©tricas: {e}")
        
        return asdict(metrics) if metrics else None

    def clear_cache(self, session_id: Optional[str] = None) -> None:
        """Limpa cache de m√©tricas"""
        if session_id:
            self.metrics_cache.pop(session_id, None)
            logger.info(f"üóëÔ∏è Cache limpo para sess√£o: {session_id}")
        else:
            self.metrics_cache.clear()
            logger.info("üóëÔ∏è Todo cache de m√©tricas limpo")

    def export_synthesis_to_formats(
        self, 
        session_id: str, 
        formats: List[str] = None
    ) -> Dict[str, str]:
        """
        Exporta s√≠ntese para diferentes formatos
        
        Args:
            session_id: ID da sess√£o
            formats: Lista de formatos desejados ['json', 'md', 'txt', 'csv']
        
        Returns:
            Dicion√°rio com caminhos dos arquivos gerados
        """
        if formats is None:
            formats = ['json', 'md']
        
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            synthesis_file = session_dir / "sintese_master_synthesis.json"
            
            if not synthesis_file.exists():
                raise FileNotFoundError("Arquivo de s√≠ntese n√£o encontrado")
            
            with open(synthesis_file, 'r', encoding='utf-8') as f:
                synthesis_data = json.load(f)
            
            exported_files = {}
            
            # JSON j√° existe
            if 'json' in formats:
                exported_files['json'] = str(synthesis_file)
            
            # Markdown
            if 'md' in formats:
                metrics = self.get_metrics(session_id)
                if metrics:
                    metrics_obj = SynthesisMetrics(**metrics)
                else:
                    metrics_obj = SynthesisMetrics(
                        context_size=0, processing_time=0, ai_searches=0,
                        data_sources=0, confidence_level=0, 
                        timestamp=datetime.now().isoformat()
                    )
                
                report = self._generate_synthesis_report(
                    synthesis_data, 
                    session_id, 
                    metrics_obj
                )
                
                md_path = session_dir / "relatorio_sintese.md"
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                exported_files['md'] = str(md_path)
            
            # Texto simples
            if 'txt' in formats:
                txt_content = self._convert_to_plain_text(synthesis_data)
                txt_path = session_dir / "sintese_resumo.txt"
                
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(txt_content)
                
                exported_files['txt'] = str(txt_path)
            
            logger.info(f"üì¶ S√≠ntese exportada em {len(exported_files)} formatos")
            return exported_files
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao exportar s√≠ntese: {e}")
            return {}

    def _convert_to_plain_text(self, synthesis_data: Dict[str, Any]) -> str:
        """Converte dados de s√≠ntese para texto simples"""
        lines = [
            "=" * 80,
            "S√çNTESE DE AN√ÅLISE - ARQV30 Enhanced v4.0",
            "=" * 80,
            "",
            "INSIGHTS PRINCIPAIS:",
            ""
        ]
        
        for i, insight in enumerate(synthesis_data.get('insights_principais', [])[:10], 1):
            lines.append(f"{i}. {insight}")
        
        lines.extend(["", "OPORTUNIDADES:", ""])
        
        for i, opp in enumerate(synthesis_data.get('oportunidades_identificadas', [])[:10], 1):
            lines.append(f"{i}. {opp}")
        
        lines.extend(["", "ESTRAT√âGIAS RECOMENDADAS:", ""])
        
        for i, strat in enumerate(synthesis_data.get('estrategias_recomendadas', [])[:10], 1):
            lines.append(f"{i}. {strat}")
        
        lines.extend(["", "=" * 80])
        
        return "\n".join(lines)

    def _analyze_synthesis_quality(self, synthesis_text: str, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa qualidade da s√≠ntese usando o sistema de qualidade integrado"""
        try:
            # Importa o sistema de qualidade
            from services.quality_system_integrator import quality_system
            
            # Determina ind√∫stria baseada no conte√∫do
            industry = self._detect_industry_from_synthesis(synthesis_text, parsed_data)
            
            # Gera ID de sess√£o para an√°lise de qualidade
            quality_session_id = f"synthesis_quality_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Executa an√°lise completa de qualidade
            quality_report = quality_system.analyze_content_quality(
                content=synthesis_text,
                industry=industry,
                session_id=quality_session_id,
                context={'synthesis_type': 'master_synthesis', 'engine': 'enhanced_v4'}
            )
            
            # Retorna dados estruturados da an√°lise
            return {
                'overall_quality_score': quality_report.overall_quality_score,
                'credibility_level': quality_report.credibility_level,
                'critical_issues_count': len(quality_report.critical_issues),
                'recommendations_count': len(quality_report.recommendations),
                'source_summary': quality_report.source_summary,
                'validation_summary': quality_report.validation_summary,
                'disclaimer_ids': quality_report.disclaimer_ids,
                'risk_analysis_available': quality_report.risk_analysis is not None,
                'regulatory_context_available': quality_report.regulatory_context is not None,
                'validation_plan_available': quality_report.validation_plan is not None,
                'quality_session_id': quality_session_id,
                'analysis_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de qualidade da s√≠ntese: {e}")
            return {
                'overall_quality_score': 0.5,
                'credibility_level': 'INDETERMINADO',
                'error': str(e),
                'analysis_timestamp': datetime.now().isoformat()
            }
    
    def _detect_industry_from_synthesis(self, synthesis_text: str, parsed_data: Dict[str, Any]) -> str:
        """Detecta ind√∫stria baseada no conte√∫do da s√≠ntese"""
        text_lower = synthesis_text.lower()
        
        # Mapeamento de palavras-chave para ind√∫strias
        industry_keywords = {
            'e_commerce': ['ecommerce', 'e-commerce', 'loja online', 'marketplace', 'vendas online'],
            'financial_services': ['financeiro', 'investimento', 'banco', 'cr√©dito', 'empr√©stimo'],
            'healthcare': ['sa√∫de', 'm√©dico', 'hospital', 'cl√≠nica', 'tratamento'],
            'technology': ['tecnologia', 'software', 'app', 'digital', 'sistema'],
            'retail': ['varejo', 'loja', 'produto', 'consumidor', 'cliente'],
            'manufacturing': ['ind√∫stria', 'produ√ß√£o', 'f√°brica', 'manufatura', 'processo'],
            'education': ['educa√ß√£o', 'ensino', 'curso', 'escola', 'aprendizado'],
            'real_estate': ['im√≥vel', 'imobili√°rio', 'casa', 'apartamento', 'propriedade']
        }
        
        # Conta ocorr√™ncias por ind√∫stria
        industry_scores = {}
        for industry, keywords in industry_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                industry_scores[industry] = score
        
        # Retorna ind√∫stria com maior score ou 'general'
        if industry_scores:
            return max(industry_scores, key=industry_scores.get)
        
        return 'general'

    def _run_external_ai_verification(self, session_id: str, synthesis_data: Dict[str, Any]) -> Dict[str, Any]:
        """Executa verifica√ß√£o externa usando o External AI Verifier"""
        try:
            # Importa o External AI Verifier
            import sys
            import os
            
            # Adiciona o caminho do external_ai_verifier ao sys.path
            external_verifier_path = os.path.join(os.path.dirname(__file__), '../../external_ai_verifier/src')
            if external_verifier_path not in sys.path:
                sys.path.append(external_verifier_path)
            
            from external_review_agent import ExternalReviewAgent
            
            # Inicializa o agente de revis√£o externa
            external_agent = ExternalReviewAgent()
            
            # Prepara dados para an√°lise
            analysis_data = {
                'items': [
                    {
                        'id': f'synthesis_{session_id}',
                        'content': json.dumps(synthesis_data, ensure_ascii=False),
                        'type': 'synthesis_result',
                        'metadata': {
                            'session_id': session_id,
                            'generated_at': datetime.now().isoformat(),
                            'engine': 'enhanced_synthesis_v4'
                        }
                    }
                ],
                'metadata': {
                    'batch_id': f'synthesis_review_{session_id}',
                    'analysis_type': 'synthesis_verification',
                    'timestamp': datetime.now().isoformat()
                }
            }
            
            # Executa an√°lise
            review_result = external_agent.analyze_content_batch(analysis_data)
            
            # Extrai m√©tricas principais
            if review_result.get('success') and review_result.get('results'):
                first_result = review_result['results'][0]
                return {
                    'success': True,
                    'overall_score': first_result.get('overall_score', 0.0),
                    'sentiment_analysis': first_result.get('sentiment_analysis', {}),
                    'bias_detection': first_result.get('bias_detection', {}),
                    'reasoning_analysis': first_result.get('reasoning_analysis', {}),
                    'contextual_analysis': first_result.get('contextual_analysis', {}),
                    'confidence_assessment': first_result.get('confidence_assessment', {}),
                    'verification_timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'success': False,
                    'error': 'Falha na an√°lise externa',
                    'verification_timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro na verifica√ß√£o externa: {e}")
            return {
                'success': False,
                'error': str(e),
                'verification_timestamp': datetime.now().isoformat()
            }


# ============================================================================
# INST√ÇNCIA GLOBAL E FUN√á√ïES AUXILIARES
# ============================================================================

# Inst√¢ncia global
enhanced_synthesis_engine = EnhancedSynthesisEngine()


# Fun√ß√µes auxiliares para uso externo
async def run_synthesis(
    session_id: str, 
    synthesis_type: str = "master_synthesis"
) -> Dict[str, Any]:
    """Fun√ß√£o auxiliar para executar s√≠ntese"""
    return await enhanced_synthesis_engine.execute_deep_specialization_study(
        session_id, 
        synthesis_type
    )


def get_synthesis_info(session_id: str) -> Dict[str, Any]:
    """Fun√ß√£o auxiliar para obter informa√ß√µes da s√≠ntese"""
    return enhanced_synthesis_engine.get_synthesis_status(session_id)


def list_synthesis_types() -> List[Dict[str, str]]:
    """Fun√ß√£o auxiliar para listar tipos dispon√≠veis"""
    return enhanced_synthesis_engine.get_available_synthesis_types()


if __name__ == "__main__":
    # Testes b√°sicos
    import sys
    
    print("üß† Enhanced Synthesis Engine v4.0")
    print("=" * 60)
    
    # Lista tipos dispon√≠veis
    print("\nTipos de S√≠ntese Dispon√≠veis:")
    for synthesis_type in list_synthesis_types():
        print(f"  - {synthesis_type['name']}: {synthesis_type['description']}")
    
    # Teste de status se session_id for fornecido
    if len(sys.argv) > 1:
        session_id = sys.argv[1]
        print(f"\nüìä Status da Sess√£o: {session_id}")
        status = get_synthesis_info(session_id)
        print(json.dumps(status, indent=2, ensure_ascii=False))