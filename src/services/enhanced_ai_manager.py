#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced AI Manager
Gerenciador de IA com hierarquia OpenRouter: Grok-4 ‚Üí Gemini-2.0 e fallbacks robustos
ZERO SIMULA√á√ÉO - Apenas modelos reais funcionais
Com delay de 10s entre requisi√ß√µes e rota√ß√£o inteligente de APIs
"""

import os
import logging
import asyncio
import json
import aiohttp
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from dotenv import load_dotenv
import time

# Carregar vari√°veis de ambiente
load_dotenv()

logger = logging.getLogger(__name__)

class EnhancedAIManager:
    """Gerenciador de IA aprimorado com hierarquia OpenRouter e fallbacks"""

    def __init__(self):
        """Inicializa o gerenciador aprimorado com hierarquia OpenRouter"""
        # Carregar chaves OpenRouter
        self.openrouter_keys = self._load_openrouter_keys()
        self.current_key_index = 0
        
        # Carregar chaves Gemini para fallback
        self.gemini_keys = self._load_gemini_keys()
        self.current_gemini_key_index = 0
        
        # Configurar hierarquia de modelos
        self.model_hierarchy = [
            {
                'name': 'x-ai/grok-4-fast:free',
                'provider': 'openrouter',
                'priority': 1,
                'max_tokens': 4000,
                'temperature': 0.7
            },
            {
                'name': 'google/gemini-2.0-flash-exp:free',
                'provider': 'openrouter',
                'priority': 2,
                'max_tokens': 8000,
                'temperature': 0.7
            },
            {
                'name': 'gemini-2.0-flash-exp',
                'provider': 'gemini_direct',
                'priority': 3,
                'max_tokens': 4000,
                'temperature': 0.7
            }
        ]
        
        # Controle de rate limiting e delays
        self.last_request_time = 0
        self.request_delay = 10  # 10 segundos entre requisi√ß√µes
        self.request_lock = asyncio.Lock()
        
        self.search_orchestrator = None
        
        # Importar search orchestrator se dispon√≠vel
        try:
            from .real_search_orchestrator import RealSearchOrchestrator
            self.search_orchestrator = RealSearchOrchestrator()
            logger.info("‚úÖ Search Orchestrator carregado")
        except ImportError:
            logger.warning("‚ö†Ô∏è Search Orchestrator n√£o dispon√≠vel")

        logger.info("ü§ñ Enhanced AI Manager inicializado com hierarquia Grok-4 ‚Üí Gemini-2.0")
        logger.info(f"üîë {len(self.openrouter_keys)} chaves OpenRouter carregadas")
        logger.info(f"üîë {len(self.gemini_keys)} chaves Gemini carregadas")
        logger.info(f"‚è±Ô∏è Delay configurado: {self.request_delay}s entre requisi√ß√µes")
    
    def _load_openrouter_keys(self) -> List[str]:
        """Carrega m√∫ltiplas chaves OpenRouter"""
        keys = []
        
        # Chave principal
        main_key = os.getenv('OPENROUTER_API_KEY')
        if main_key and main_key.strip():
            keys.append(main_key.strip())
            
        # Chaves numeradas
        for i in range(1, 6):
            key = os.getenv(f'OPENROUTER_API_KEY_{i}')
            if key and key.strip():
                keys.append(key.strip())
                
        logger.info(f"‚úÖ {len(keys)} chaves OpenRouter carregadas")
        return keys
    
    def _load_gemini_keys(self) -> List[str]:
        """Carrega m√∫ltiplas chaves Gemini"""
        keys = []
        
        # Chave principal
        main_key = os.getenv('GEMINI_API_KEY')
        if main_key and main_key.strip():
            keys.append(main_key.strip())
            
        # Chaves numeradas
        for i in range(1, 4):
            key = os.getenv(f'GEMINI_API_KEY_{i}')
            if key and key.strip():
                keys.append(key.strip())
                
        logger.info(f"‚úÖ {len(keys)} chaves Gemini carregadas")
        return keys
    
    def _get_next_openrouter_key(self) -> Optional[str]:
        """Obt√©m pr√≥xima chave OpenRouter com rota√ß√£o"""
        if not self.openrouter_keys:
            return None
            
        key = self.openrouter_keys[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(self.openrouter_keys)
        logger.info(f"üîÑ Rotacionando para chave OpenRouter #{self.current_key_index + 1}/{len(self.openrouter_keys)}")
        return key
    
    def _get_next_gemini_key(self) -> Optional[str]:
        """Obt√©m pr√≥xima chave Gemini com rota√ß√£o"""
        if not self.gemini_keys:
            return None
            
        key = self.gemini_keys[self.current_gemini_key_index]
        self.current_gemini_key_index = (self.current_gemini_key_index + 1) % len(self.gemini_keys)
        logger.info(f"üîÑ Rotacionando para chave Gemini #{self.current_gemini_key_index + 1}/{len(self.gemini_keys)}")
        return key

    async def _apply_rate_limit_delay(self):
        """Aplica delay de 10 segundos entre requisi√ß√µes"""
        async with self.request_lock:
            current_time = time.time()
            time_since_last_request = current_time - self.last_request_time
            
            if time_since_last_request < self.request_delay:
                wait_time = self.request_delay - time_since_last_request
                logger.info(f"‚è±Ô∏è Aguardando {wait_time:.2f}s antes da pr√≥xima requisi√ß√£o...")
                await asyncio.sleep(wait_time)
            
            self.last_request_time = time.time()

    async def _generate_with_openrouter(
        self,
        prompt: str,
        model_name: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """Gera conte√∫do usando OpenRouter com rota√ß√£o de chaves e delay"""
        
        # Preparar mensagens
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # Tentar com todas as chaves dispon√≠veis
        for attempt in range(len(self.openrouter_keys)):
            # Aplicar delay antes de cada requisi√ß√£o
            await self._apply_rate_limit_delay()
            
            api_key = self._get_next_openrouter_key()
            if not api_key:
                continue
                
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/joscarmao/v1800finalv2",
                    "X-Title": "ARQV30 Enhanced v3.0"
                }
                
                payload = {
                    "model": model_name,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": False
                }
                
                logger.info(f"üì§ Enviando requisi√ß√£o para OpenRouter ({model_name}) - Tentativa {attempt + 1}/{len(self.openrouter_keys)}")
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://openrouter.ai/api/v1/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=120)
                    ) as response:
                        
                        if response.status == 200:
                            result = await response.json()
                            content = result["choices"][0]["message"]["content"]
                            logger.info(f"‚úÖ OpenRouter {model_name} sucesso (chave #{self.current_key_index})")
                            return content
                        else:
                            error_text = await response.text()
                            logger.warning(f"‚ö†Ô∏è OpenRouter key {attempt + 1} falhou: {response.status} - {error_text[:200]}")
                            
            except asyncio.TimeoutError:
                logger.warning(f"‚è±Ô∏è Timeout na requisi√ß√£o OpenRouter key {attempt + 1}")
                continue
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro OpenRouter key {attempt + 1}: {str(e)[:100]}")
                continue
        
        logger.error(f"‚ùå Todas as {len(self.openrouter_keys)} chaves OpenRouter falharam para {model_name}")
        return None
    
    async def _generate_with_gemini_direct(
        self,
        prompt: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """Gera conte√∫do usando Gemini direto com rota√ß√£o de chaves e delay"""
        
        try:
            import google.generativeai as genai
            
            # Tentar com todas as chaves Gemini
            for attempt in range(len(self.gemini_keys)):
                # Aplicar delay antes de cada requisi√ß√£o
                await self._apply_rate_limit_delay()
                
                api_key = self._get_next_gemini_key()
                if not api_key:
                    continue
                    
                try:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel("gemini-2.0-flash-exp")
                    
                    # Combinar system prompt e user prompt se necess√°rio
                    full_prompt = prompt
                    if system_prompt:
                        full_prompt = f"{system_prompt}\n\n{prompt}"
                    
                    generation_config = {
                        'temperature': temperature,
                        'top_p': 0.95,
                        'top_k': 64,
                        'max_output_tokens': max_tokens,
                    }
                    
                    logger.info(f"üì§ Enviando requisi√ß√£o para Gemini Direct - Tentativa {attempt + 1}/{len(self.gemini_keys)}")
                    
                    response = model.generate_content(
                        full_prompt,
                        generation_config=generation_config
                    )
                    
                    if response.text:
                        logger.info(f"‚úÖ Gemini direto sucesso (chave #{self.current_gemini_key_index})")
                        return response.text
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro Gemini key {attempt + 1}: {str(e)[:100]}")
                    continue
            
            logger.error(f"‚ùå Todas as {len(self.gemini_keys)} chaves Gemini falharam")
            return None
            
        except ImportError:
            logger.error("‚ùå google-generativeai n√£o instalado")
            return None
    
    def generate_response(
        self,
        prompt: str,
        model: str = "x-ai/grok-4-fast:free",
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Gera resposta s√≠ncrona usando hierarquia de modelos"""
        try:
            # Executar gera√ß√£o ass√≠ncrona de forma s√≠ncrona
            import asyncio
            
            async def _async_generate():
                return await self.generate_text(
                    prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    model_override=model
                )
            
            # Tentar obter loop existente ou criar novo
            try:
                loop = asyncio.get_running_loop()
                # Se j√° h√° um loop rodando, criar task
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, _async_generate())
                    content = future.result(timeout=180)
            except RuntimeError:
                # Nenhum loop rodando, executar diretamente
                content = asyncio.run(_async_generate())
            
            return {
                'success': True,
                'content': content,
                'model': model,
                'provider': 'hierarchy',
                'tokens_used': len(content.split()) * 1.3  # Estimativa
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de resposta: {e}")
            return {
                'success': False,
                'content': 'Erro interno ao gerar resposta',
                'error': str(e)
            }

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """
        Gera texto usando hierarquia de modelos: Grok-4 ‚Üí Gemini-2.0 ‚Üí Gemini Direct
        Com delay de 10s entre requisi√ß√µes e rota√ß√£o de APIs
        
        Args:
            prompt: Prompt do usu√°rio
            system_prompt: Prompt do sistema (opcional)
            max_tokens: M√°ximo de tokens (opcional)
            temperature: Temperatura (opcional)
            model_override: Modelo espec√≠fico (opcional)
        
        Returns:
            String com a resposta da IA
        """
        max_tokens = max_tokens or 4000
        temperature = temperature or 0.7
        
        # Se modelo espec√≠fico foi solicitado, tentar apenas ele
        if model_override:
            target_models = [m for m in self.model_hierarchy if m['name'] == model_override]
            if not target_models:
                # Se modelo n√£o encontrado, usar hierarquia normal
                target_models = self.model_hierarchy
        else:
            target_models = self.model_hierarchy
        
        # Tentar cada modelo na hierarquia
        for model_config in target_models:
            try:
                logger.info(f"ü§ñ Tentando {model_config['name']} ({model_config['provider']})")
                
                if model_config['provider'] == 'openrouter':
                    result = await self._generate_with_openrouter(
                        prompt=prompt,
                        model_name=model_config['name'],
                        max_tokens=min(max_tokens, model_config['max_tokens']),
                        temperature=temperature,
                        system_prompt=system_prompt
                    )
                    
                elif model_config['provider'] == 'gemini_direct':
                    result = await self._generate_with_gemini_direct(
                        prompt=prompt,
                        max_tokens=min(max_tokens, model_config['max_tokens']),
                        temperature=temperature,
                        system_prompt=system_prompt
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è Provider desconhecido: {model_config['provider']}")
                    continue
                
                if result:
                    logger.info(f"‚úÖ Sucesso com {model_config['name']}")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è {model_config['name']} n√£o retornou resultado")
                    
            except Exception as e:
                logger.error(f"‚ùå Erro com {model_config['name']}: {str(e)[:100]}")
                continue
        
        # Se todos os modelos falharam, usar fallback b√°sico
        logger.error("‚ùå Todos os modelos da hierarquia falharam")
        raise Exception("Todos os modelos de IA falharam. Verifique as configura√ß√µes das APIs.")
    
    def generate_text_sync(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """Vers√£o s√≠ncrona da gera√ß√£o de texto"""
        try:
            import asyncio
            
            async def _async_wrapper():
                return await self.generate_text(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    model_override=model_override
                )
            
            try:
                loop = asyncio.get_running_loop()
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, _async_wrapper())
                    return future.result(timeout=180)
            except RuntimeError:
                return asyncio.run(_async_wrapper())
                
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"‚ùå Erro de conex√£o ao gerar texto (sync): {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"‚ùå Erro de par√¢metros ao gerar texto (sync): {str(e)}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao gerar texto (sync): {str(e)}")
            raise

    async def _perform_smart_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Realiza busca inteligente com fallbacks Serper ‚Üí Jina ‚Üí EXA"""
        
        if not self.search_orchestrator:
            logger.warning("‚ö†Ô∏è Search Orchestrator n√£o dispon√≠vel")
            return []
        
        try:
            # 1. Tentar Serper primeiro
            logger.info(f"üîç Tentando busca Serper para: {query}")
            serper_results = await self.search_orchestrator.search_serper(query, max_results)
            
            if serper_results:
                logger.info(f"‚úÖ Serper retornou {len(serper_results)} resultados")
                return serper_results
            
            # 2. Fallback para Jina
            logger.info(f"üîç Fallback: Tentando busca Jina para: {query}")
            jina_results = await self.search_orchestrator.search_jina(query, max_results)
            
            if jina_results:
                logger.info(f"‚úÖ Jina retornou {len(jina_results)} resultados")
                return jina_results
            
            # 3. Fallback para EXA
            logger.info(f"üîç Fallback: Tentando busca EXA para: {query}")
            exa_results = await self.search_orchestrator.search_exa(query, max_results)
            
            if exa_results:
                logger.info(f"‚úÖ EXA retornou {len(exa_results)} resultados")
                return exa_results
            
            logger.warning("‚ö†Ô∏è Todos os servi√ßos de busca falharam")
            return []
            
        except Exception as e:
            logger.error(f"‚ùå Erro na busca inteligente: {e}")
            return []

    async def generate_with_active_search(
        self,
        prompt: str,
        context: str = "",
        session_id: str = None,
        max_search_iterations: int = 3,
        preferred_model: str = None,
        min_processing_time: int = 0
    ) -> str:
        """
        Gera conte√∫do com busca ativa usando hierarquia Grok-4 ‚Üí Gemini
        Com delay de 10s entre requisi√ß√µes
        """
        logger.info(f"üîç Iniciando gera√ß√£o com busca ativa (modelo: {preferred_model or 'hierarquia'})")
        
        # Registrar tempo de in√≠cio para garantir tempo m√≠nimo
        start_time = datetime.now()

        # Realizar buscas complementares se necess√°rio
        additional_context = ""
        if max_search_iterations > 0:
            # Extrair termos de busca do prompt
            search_queries = self._extract_search_terms(prompt)
            
            for i, query in enumerate(search_queries[:max_search_iterations]):
                logger.info(f"üîç Busca {i+1}/{len(search_queries)}: {query}")
                search_results = await self._perform_smart_search(query, max_results=3)
                
                if search_results:
                    additional_context += f"\n\n=== DADOS DE BUSCA: {query} ===\n"
                    for result in search_results:
                        additional_context += f"- {result.get('title', 'Sem t√≠tulo')}: {result.get('snippet', result.get('description', ''))}\n"

        # Prepara prompt com instru√ß√µes de busca e contexto
        enhanced_prompt = f"""
{prompt}

CONTEXTO PRINCIPAL:
{context}

{additional_context if additional_context else ""}

INSTRU√á√ïES ESPECIAIS:
- Analise o contexto fornecido detalhadamente
- Use os dados de busca complementares para enriquecer a an√°lise
- Procure por estat√≠sticas, tend√™ncias e casos reais
- Forne√ßa insights profundos baseados nos dados dispon√≠veis
- Combine informa√ß√µes de m√∫ltiplas fontes para criar an√°lise robusta

IMPORTANTE: Gere uma an√°lise completa e profissional baseando-se em TODOS os dados fornecidos.
"""

        # Sistema prompt para busca ativa
        system_prompt = """Voc√™ √© um especialista em an√°lise de mercado e tend√™ncias digitais com acesso a dados em tempo real.
        Sua fun√ß√£o √© gerar an√°lises profundas e insights valiosos baseados nos dados fornecidos.
        Sempre forne√ßa informa√ß√µes precisas, atualizadas e acion√°veis.
        Combine dados de m√∫ltiplas fontes para criar an√°lises robustas e confi√°veis."""

        try:
            # Usar modelo preferido ou hierarquia
            logger.info(f"ü§ñ Gerando com modelo: {preferred_model or 'hierarquia Grok-4 ‚Üí Gemini'}")
            
            # Gerar resposta usando hierarquia
            response = await self.generate_text(
                prompt=enhanced_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7,
                model_override=preferred_model
            )
            
            # Garantir tempo m√≠nimo de processamento se especificado
            if min_processing_time > 0:
                elapsed_time = (datetime.now() - start_time).total_seconds()
                if elapsed_time < min_processing_time:
                    remaining_time = min_processing_time - elapsed_time
                    logger.info(f"‚è±Ô∏è Aguardando {remaining_time:.1f}s para completar tempo m√≠nimo")
                    await asyncio.sleep(remaining_time)
            
            logger.info("‚úÖ Gera√ß√£o com busca ativa conclu√≠da")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o com busca ativa: {e}")
            # Fallback simples
            try:
                return await self.generate_text(enhanced_prompt, system_prompt)
            except Exception as e2:
                logger.error(f"‚ùå Erro no fallback: {e2}")
                raise
    
    def _extract_search_terms(self, prompt: str) -> List[str]:
        """Extrai termos de busca relevantes do prompt"""
        # Implementa√ß√£o b√°sica - pode ser melhorada
        search_terms = []
        
        # Buscar por palavras-chave comuns
        keywords = ['mercado', 'brasil', 'tend√™ncias', 'estat√≠sticas', 'dados', 'an√°lise']
        
        for keyword in keywords:
            if keyword in prompt.lower():
                # Extrair contexto ao redor da palavra-chave
                words = prompt.lower().split()
                for i, word in enumerate(words):
                    if keyword in word:
                        # Pegar 2 palavras antes e depois
                        start = max(0, i-2)
                        end = min(len(words), i+3)
                        search_term = ' '.join(words[start:end])
                        search_terms.append(search_term)
                        break
        
        # Se n√£o encontrou termos espec√≠ficos, usar primeiras palavras
        if not search_terms:
            words = prompt.split()[:5]
            search_terms.append(' '.join(words))
        
        return search_terms[:3]  # M√°ximo 3 buscas

    async def analyze_content(
        self,
        content: str,
        analysis_type: str = "comprehensive",
        target_audience: str = "general",
        model_preference: str = None
    ) -> str:
        """
        Analisa conte√∫do usando hierarquia OpenRouter com delay
        
        Args:
            content: Conte√∫do para an√°lise
            analysis_type: Tipo de an√°lise (comprehensive, viral, market, etc.)
            target_audience: P√∫blico-alvo
            model_preference: Prefer√™ncia de modelo
        
        Returns:
            An√°lise detalhada do conte√∫do
        """
        system_prompt = f"""Voc√™ √© um especialista em an√°lise de conte√∫do digital e marketing.
        Sua fun√ß√£o √© analisar conte√∫do de forma {analysis_type} para o p√∫blico {target_audience}.
        Forne√ßa insights acion√°veis, tend√™ncias identificadas e recomenda√ß√µes estrat√©gicas."""
        
        analysis_prompt = f"""
Analise o seguinte conte√∫do de forma {analysis_type}:

CONTE√öDO:
{content}

P√öBLICO-ALVO: {target_audience}

FORNE√áA:
1. An√°lise detalhada do conte√∫do
2. Pontos fortes e fracos identificados
3. Potencial viral e engajamento
4. Recomenda√ß√µes de melhoria
5. Estrat√©gias de distribui√ß√£o
6. Insights de mercado relevantes

Seja espec√≠fico, pr√°tico e acion√°vel em suas recomenda√ß√µes.
"""
        
        try:
            return await self.generate_text(
                prompt=analysis_prompt,
                system_prompt=system_prompt,
                max_tokens=3000,
                temperature=0.7,
                model_override=model_preference
            )
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de conte√∫do: {e}")
            raise

    async def generate_insights(
        self,
        data: Dict[str, Any],
        insight_type: str = "market_trends",
        depth: str = "deep"
    ) -> str:
        """
        Gera insights baseados em dados usando hierarquia OpenRouter com delay
        
        Args:
            data: Dados para an√°lise
            insight_type: Tipo de insight desejado
            depth: Profundidade da an√°lise (shallow, medium, deep)
        
        Returns:
            Insights gerados
        """
        system_prompt = f"""Voc√™ √© um analista de dados especializado em {insight_type}.
        Sua fun√ß√£o √© gerar insights {depth} baseados nos dados fornecidos.
        Sempre forne√ßa an√°lises precisas, tend√™ncias identificadas e recomenda√ß√µes acion√°veis."""
        
        data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        insights_prompt = f"""
Analise os seguintes dados e gere insights {depth} sobre {insight_type}:

DADOS:
{data_str}

FORNE√áA:
1. Principais tend√™ncias identificadas
2. Padr√µes e correla√ß√µes importantes
3. Oportunidades de mercado
4. Riscos e desafios
5. Recomenda√ß√µes estrat√©gicas
6. Previs√µes baseadas nos dados

Seja espec√≠fico, use n√∫meros quando relevante e forne√ßa insights acion√°veis.
"""
        
        try:
            return await self.generate_text(
                prompt=insights_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.6
            )
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de insights: {e}")
            raise

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do gerenciador"""
        return {
            "openrouter_keys_count": len(self.openrouter_keys),
            "gemini_keys_count": len(self.gemini_keys),
            "current_openrouter_key_index": self.current_key_index,
            "current_gemini_key_index": self.current_gemini_key_index,
            "request_delay_seconds": self.request_delay,
            "last_request_time": self.last_request_time,
            "search_orchestrator_available": self.search_orchestrator is not None,
            "model_hierarchy": [m['name'] for m in self.model_hierarchy],
            "timestamp": datetime.now().isoformat()
        }

    def reset_failed_models(self):
        """Reseta √≠ndices de rota√ß√£o de chaves"""
        self.current_key_index = 0
        self.current_gemini_key_index = 0
        self.last_request_time = 0
        logger.info("‚úÖ √çndices de rota√ß√£o resetados")

# Inst√¢ncia global para uso em todo o projeto
enhanced_ai_manager = EnhancedAIManager()

# Fun√ß√µes de conveni√™ncia para uso direto
async def generate_ai_text(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """Fun√ß√£o de conveni√™ncia para gera√ß√£o de texto"""
    return await enhanced_ai_manager.generate_text(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model_override=model_override
    )

def generate_ai_text_sync(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """Fun√ß√£o de conveni√™ncia s√≠ncrona para gera√ß√£o de texto"""
    return enhanced_ai_manager.generate_text_sync(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model_override=model_override
    )

if __name__ == "__main__":
    # Teste b√°sico
    async def test():
        try:
            manager = EnhancedAIManager()
            
            print("üß™ Testando gera√ß√£o de texto com delay e rota√ß√£o de APIs...")
            print(f"‚è±Ô∏è Delay configurado: {manager.request_delay}s entre requisi√ß√µes")
            print(f"üîë Chaves OpenRouter dispon√≠veis: {len(manager.openrouter_keys)}")
            print(f"üîë Chaves Gemini dispon√≠veis: {len(manager.gemini_keys)}")
            print()
            
            # Teste 1: Gera√ß√£o simples
            print("üìù Teste 1: Gera√ß√£o de texto simples")
            response1 = await manager.generate_text(
                prompt="Explique brevemente o que √© intelig√™ncia artificial",
                system_prompt="Voc√™ √© um especialista em tecnologia"
            )
            print(f"‚úÖ Resposta 1 (primeiros 200 chars): {response1[:200]}...")
            print()
            
            # Teste 2: Segunda requisi√ß√£o (deve aguardar 10s)
            print("üìù Teste 2: Segunda requisi√ß√£o (testando delay)")
            response2 = await manager.generate_text(
                prompt="O que √© machine learning?",
                system_prompt="Voc√™ √© um especialista em IA"
            )
            print(f"‚úÖ Resposta 2 (primeiros 200 chars): {response2[:200]}...")
            print()
            
            # Teste 3: Status do gerenciador
            print("üìä Status do gerenciador:")
            status = manager.get_status()
            print(json.dumps(status, indent=2, default=str, ensure_ascii=False))
            print()
            
            print("‚úÖ Todos os testes conclu√≠dos com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro no teste: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(test())
